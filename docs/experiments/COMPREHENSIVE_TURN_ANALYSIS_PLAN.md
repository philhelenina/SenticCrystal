# SenticCrystal ì¢…í•© Turn Analysis ì‹¤í—˜ ê³„íš

## ğŸ¯ **ì‹¤í—˜ ëª©í‘œ**

Config146 ì„¤ì • + Forward-only Turn Context Window ì „ëµì„ í†µí•œ ê°ì • ì¸ì‹ ì„±ëŠ¥ ìµœì í™”

## ğŸ“Š **Phase 1: Turn Analysis (í˜„ì¬ ë‹¨ê³„)**

### **1.1 ì„ë² ë”© ìƒì„±**
**Input:**
- IEMOCAP 4-way ë°ì´í„° (train/val/test)
- Config146 ì„¤ì •:
  ```json
  {
    "apply_word_pe": false,
    "pooling_method": "weighted_mean", 
    "apply_sentence_pe": false,
    "combination_method": "sum",
    "bayesian_method": "context_lstm"
  }
  ```
- WordNet-Affect (300ì°¨ì›) + Sentence-RoBERTa (768ì°¨ì›)
- Forward-only context: í˜„ì¬ + ì´ì „ K-1ê°œ í„´ë§Œ ì‚¬ìš©

**Output:**
- Config146 ì„ë² ë”© íŒŒì¼ë“¤:
  ```
  embeddings/config146_proper/
  â”œâ”€â”€ 0turn/  # K=0 (í˜„ì¬ utteranceë§Œ)
  â”œâ”€â”€ 2turn/  # K=2 (ì´ì „ 1ê°œ + í˜„ì¬)
  â”œâ”€â”€ 4turn/  # K=4 (ì´ì „ 3ê°œ + í˜„ì¬) 
  â””â”€â”€ 6turn/  # K=6 (ì´ì „ 5ê°œ + í˜„ì¬)
  ```

### **1.2 Context Window ì „ëµ êµ¬í˜„**

**ì „ëµ 1: Cumulative Quantile**
- ê° ëŒ€í™”ë³„ temporal position ê¸°ë°˜
- Q1 (0-25%): K â‰¤ 8
- Q2 (25-50%): K â‰¤ 15  
- Q3 (50-75%): K â‰¤ 25
- Q4 (75-100%): K â‰¤ 35

**ì „ëµ 2: Pure Cumulative**
- K = utterance_position (dialogue ë‚´ ìœ„ì¹˜)
- ì²˜ìŒë¶€í„° í˜„ì¬ê¹Œì§€ ëª¨ë“  ì´ì „ context ì‚¬ìš©

**ì „ëµ 3: Conservative Cumulative**  
- Pure cumulativeì˜ ë³´ìˆ˜ì  ë²„ì „
- K = min(utterance_position, max_K_limit)

**ì „ëµ 4: Fixed Baselines**
- K=0, K=2, K=4, K=6 ê³ ì •ê°’

### **1.3 ë¶„ë¥˜ ëª¨ë¸ Architecture**

**ê¸°ë³¸ MLP ë¶„ë¥˜ê¸°:**
```python
MLPClassifier(
    input_size=768,          # S-RoBERTa + WNA (sum ê²°í•©)
    hidden_size=256,         # ì²« ë²ˆì§¸ hidden layer
    hidden_size2=128,        # ë‘ ë²ˆì§¸ hidden layer  
    num_classes=4,           # 4-way classification
    dropout=0.3,             # Regularization
    activation='ReLU'        # Activation function
)

# Training Configuration
optimizer = Adam(lr=0.001)   # Learning rate
loss = CrossEntropyLoss()    # Loss function
batch_size = 32              # Batch size
max_epochs = 200             # Maximum epochs
early_stopping = 10          # Patience
```

### **1.4 í‰ê°€ ë©”íŠ¸ë¦­**

**ëª¨ë“  ì „ëµì— ëŒ€í•´ ë‹¤ìŒ ë©”íŠ¸ë¦­ ìˆ˜ì§‘:**
1. **Accuracy** - ì „ì²´ ì •í™•ë„
2. **Macro F1-Score** - í´ë˜ìŠ¤ë³„ F1ì˜ í‰ê·  
3. **Weighted F1-Score** - í´ë˜ìŠ¤ ë¹ˆë„ë¡œ ê°€ì¤‘ëœ F1
4. **Per-class F1** - Angry, Happy, Sad, Neutral ê°ê°
5. **Confusion Matrix** - 4x4 í˜¼ë™ í–‰ë ¬
6. **Learning Curves** - Train/Val loss, accuracy ê³¡ì„ 

### **1.5 í†µê³„ì  ê²€ì¦**
- **Multiple Random Seeds** (3-5íšŒ ì‹¤í–‰)
- **Statistical Significance Testing** (t-test)
- **Error Analysis** - í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„

## ğŸ“ˆ **Expected Results**

**Target Performance Table:**
| Strategy | Accuracy | Macro-F1 | Weighted-F1 | ê¸°ëŒ€ ì„±ëŠ¥ |
|----------|----------|----------|-------------|---------|
| Fixed K=0 | ~0.66 | ~0.66 | ~0.66 | Baseline |
| Fixed K=2 | ~0.67 | ~0.67 | ~0.67 | Baseline |
| Fixed K=4 | ~0.70 | ~0.70 | ~0.70 | Baseline |
| Fixed K=6 | ~0.72 | ~0.72 | ~0.72 | Current Best |
| **Cumulative Quantile** | **~0.75** | **~0.75** | **~0.76** | **Target** |
| **Pure Cumulative** | **~0.75** | **~0.75** | **~0.76** | **Target** |
| **Conservative Cumulative** | **~0.74** | **~0.74** | **~0.75** | **Target** |

## ğŸ“Š **Output ê²°ê³¼ë¬¼**

### **1. ì„±ëŠ¥ ë¶„ì„ íŒŒì¼ë“¤**
```
results/turn_analysis_20250910/
â”œâ”€â”€ comprehensive_results.json          # ëª¨ë“  ë©”íŠ¸ë¦­ ê²°ê³¼
â”œâ”€â”€ statistical_analysis.json           # í†µê³„ì  ìœ ì˜ì„± ë¶„ì„
â”œâ”€â”€ per_class_performance.json          # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë¶„ì„
â””â”€â”€ confusion_matrices.json             # ëª¨ë“  ì „ëµì˜ í˜¼ë™í–‰ë ¬
```

### **2. ì‹œê°í™” ìë£Œë“¤**
```
figures/turn_analysis/
â”œâ”€â”€ performance_comparison.png          # ì „ëµë³„ ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸
â”œâ”€â”€ learning_curves_all_strategies.png  # ëª¨ë“  ì „ëµì˜ í•™ìŠµ ê³¡ì„ 
â”œâ”€â”€ confusion_matrices_grid.png         # í˜¼ë™í–‰ë ¬ grid ì‹œê°í™”
â”œâ”€â”€ statistical_significance.png        # í†µê³„ì  ìœ ì˜ì„± ì‹œê°í™”
â””â”€â”€ per_class_f1_breakdown.png         # í´ë˜ìŠ¤ë³„ F1 ë¶„í•´ ë¶„ì„
```

### **3. ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ**
```
TURN_ANALYSIS_COMPREHENSIVE_RESULTS.md
â”œâ”€â”€ Executive Summary
â”œâ”€â”€ Methodology Details  
â”œâ”€â”€ Performance Results
â”œâ”€â”€ Statistical Validation
â”œâ”€â”€ Error Analysis
â”œâ”€â”€ Implementation Specifications
â””â”€â”€ Next Steps for Optimization
```

## ğŸš€ **Phase 2: Optimization (ì´í›„ ë‹¨ê³„)**

Phase 1 ì™„ë£Œ í›„ ì§„í–‰í•  ìµœì í™” ì „ëµ:

### **2.1 Bayesian Hyperparameter Optimization**
- **Target Parameters:**
  - Learning rate scheduling
  - Architecture depth/width
  - Dropout rates
  - Batch size optimization

### **2.2 Loss Function Optimization** 
- **Focal Loss** with gamma parameter tuning
- **Label Smoothing** for confidence calibration  
- **Weighted CrossEntropy** for class balancing

### **2.3 Architecture Comparison**
- **MLP vs LSTM** classifier ë¹„êµ
- **Attention mechanisms** ì¶”ê°€
- **Residual connections** ì ìš©

### **2.4 Performance Probing**
- **High-performance region** íƒìƒ‰
- **Ensemble methods** ì ìš©
- **Model fusion** strategies

## â±ï¸ **ì‹¤í–‰ ê³„íš**

1. **Step 1** (2-3ì‹œê°„): Config146 ì„ë² ë”© ìƒì„± (ëª¨ë“  Kê°’)
2. **Step 2** (3-4ì‹œê°„): Cumulative context ì „ëµ êµ¬í˜„ ë° ì‹¤í—˜
3. **Step 3** (1-2ì‹œê°„): ì¢…í•© ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”
4. **Step 4** (1ì‹œê°„): ë³´ê³ ì„œ ë° ë¬¸ì„œí™”

**Total Estimated Time: 7-10ì‹œê°„**

## ğŸ¯ **Success Criteria**

âœ… **Phase 1 ì„±ê³µ ê¸°ì¤€:**
- [ ] ëª¨ë“  turn ì „ëµ ì‹¤í—˜ ì™„ë£Œ
- [ ] í†µê³„ì  ìœ ì˜ì„± í™•ì¸ (+3% ì´ìƒ ì„±ëŠ¥ í–¥ìƒ)
- [ ] ëª¨ë“  ë©”íŠ¸ë¦­ ë° ì‹œê°í™” ìë£Œ ìƒì„±
- [ ] ì¬í˜„ ê°€ëŠ¥í•œ êµ¬í˜„ ë° ë¬¸ì„œí™”

âœ… **Ready for Phase 2:**
- [ ] Baseline ëŒ€ë¹„ ì„±ëŠ¥ í–¥ìƒ í™•ì¸
- [ ] ìµœì  turn ì „ëµ ì‹ë³„
- [ ] Optimization ë°©í–¥ì„± ê²°ì •