# Saturn Cloud A100 Setup Guide for SenticCrystal

## ğŸš€ **Quick Setup**

### **1. Saturn Cloud ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ìƒì„±**
```bash
# Saturn Cloudì—ì„œ ìƒˆ ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ìƒì„±
- Resource: A100 SXM4-80GB (2 GPUs ê¶Œì¥)
- Instance Type: ml.p4d.2xlarge ë˜ëŠ” ml.p4d.4xlarge
- Storage: 500GB+ EBS ë³¼ë¥¨
- Environment: Custom (ì•„ë˜ ì„¤ì • ì‚¬ìš©)
```

### **2. Environment ì„¤ì •**
```bash
# í”„ë¡œì íŠ¸ë¥¼ Saturn Cloudë¡œ ì—…ë¡œë“œ
git clone <your-senticcrystal-repo>
cd SenticCrystal

# Conda í™˜ê²½ ìƒì„±
conda env create -f environment_saturn_cloud.yml
conda activate senticcrystal-saturn
```

### **3. ë°ì´í„° ì¤€ë¹„**
```bash
# IEMOCAP ë°ì´í„°ë¥¼ Saturn Cloud storageë¡œ ì—…ë¡œë“œ
# ë˜ëŠ” S3/GCSì—ì„œ ë‹¤ìš´ë¡œë“œ ì„¤ì •
```

## âš¡ **Performance ìµœì í™” ì„¤ì •**

### **A100 ë“€ì–¼ GPU ì„¤ì •**
```python
# PyTorch ì„¤ì • í™•ì¸
import torch
print(f"Available GPUs: {torch.cuda.device_count()}")
print(f"Current device: {torch.cuda.current_device()}")

# Multi-GPU í™œìš©
if torch.cuda.device_count() > 1:
    model = torch.nn.DataParallel(model)
    print("Multi-GPU mode enabled")
```

### **ë©”ëª¨ë¦¬ ìµœì í™”**
```bash
# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (environment.ymlì— í¬í•¨ë¨)
export PYTORCH_CUDA_ALLOC_CONF="max_split_size_mb:2048"
export CUDA_LAUNCH_BLOCKING=0
```

### **ë°°ì¹˜ í¬ê¸° ê¶Œì¥ì‚¬í•­**
```python
# A100 80GB Ã— 2 = 160GB VRAM ê¸°ì¤€
training_configs = {
    "embedding_generation": {
        "batch_size": 64,      # S-RoBERTa ì²˜ë¦¬
        "context_turns": 6,    # K-turn windows  
    },
    "model_training": {
        "batch_size": 128,     # MLP ë¶„ë¥˜ê¸°
        "max_epochs": 200,
        "early_stopping": 10
    },
    "bayesian_training": {
        "batch_size": 64,      # ë©”ëª¨ë¦¬ ë” ë§ì´ ì‚¬ìš©
        "mc_samples": 10,      # Monte Carlo ìƒ˜í”Œ
    }
}
```

## ğŸ”¬ **ì‹¤í—˜ ì‹¤í–‰ ê°€ì´ë“œ**

### **ì „ì²´ ì‹¤í—˜ íŒŒì´í”„ë¼ì¸**
```bash
# 1. Config146 ì„ë² ë”© ìƒì„± (ì˜ˆìƒ: 30-45ë¶„)
python scripts/embeddings.py --config config146 --context_turns 0,2,4,6

# 2. ì¢…í•© ì‹¤í—˜ ì‹¤í–‰ (ì˜ˆìƒ: 1.5-2ì‹œê°„)  
python run_comprehensive_experiments.py --gpu_ids 0,1

# 3. ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”
python analyze_results.py --results_dir results/
```

### **ë¶„ì‚° ì²˜ë¦¬ í™œìš©**
```python
# Daskë¥¼ í™œìš©í•œ ë³‘ë ¬ ì²˜ë¦¬ (environment.ymlì— í¬í•¨)
import dask
from dask.distributed import Client

client = Client('scheduler-address')  # Saturn Cloud Dask í´ëŸ¬ìŠ¤í„°
```

## ğŸ“Š **ì˜ˆìƒ ì„±ëŠ¥**

### **Saturn Cloud A100 vs MacBook M4**
| ì‘ì—… | MacBook M4 | Saturn A100 | ê°€ì† ë¹„ìœ¨ |
|-----|-----------|-------------|----------|
| Config146 ì„ë² ë”© (K=6) | 2-3ì‹œê°„ | 30-45ë¶„ | 4-5x |
| MLP í›ˆë ¨ | 15-20ë¶„ | 3-5ë¶„ | 4-5x |
| Bayesian í›ˆë ¨ | 45-60ë¶„ | 8-12ë¶„ | 5-6x |
| ì „ì²´ íŒŒì´í”„ë¼ì¸ | 6-8ì‹œê°„ | 1.5-2ì‹œê°„ | 4x |

### **ë¹„ìš© ìµœì í™”**
```bash
# Auto-shutdown ì„¤ì • (ìœ ë£Œ ì‹œê°„ ì ˆì•½)
# Saturn Cloud UIì—ì„œ ì„¤ì •:
- Idle timeout: 30ë¶„
- Auto-shutdown: 2ì‹œê°„ ìœ íœ´ì‹œ
- ì‹¤í—˜ ì™„ë£Œ í›„ ìˆ˜ë™ ì¢…ë£Œ ê¶Œì¥
```

## ğŸ› ï¸ **Troubleshooting**

### **CUDA ë©”ëª¨ë¦¬ ë¶€ì¡±ì‹œ**
```python
# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
batch_size = 32  # 64ì—ì„œ 32ë¡œ
context_turns = 4  # 6ì—ì„œ 4ë¡œ

# ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ì‚¬ìš©
accumulation_steps = 4
effective_batch_size = batch_size * accumulation_steps
```

### **ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ì‹œ**
```python
# ìºì‹œ í´ë¦¬ì–´
import torch
torch.cuda.empty_cache()

# HuggingFace ìºì‹œ ì¬ì„¤ì •
export HF_HOME="/tmp/huggingface_new"
```

### **ë„¤íŠ¸ì›Œí¬ ì—°ê²° ë¬¸ì œì‹œ**
```bash
# ëª¨ë¸ ì‚¬ì „ ë‹¤ìš´ë¡œë“œ
python -c "
from sentence_transformers import SentenceTransformer
SentenceTransformer('all-MiniLM-L6-v2')
print('Model cached')
"
```

## ğŸ“ˆ **ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…**

### **ì‹¤í—˜ ì¶”ì **
```python
# W&B ì„¤ì • (environment.ymlì— í¬í•¨)
import wandb
wandb.init(project="senticcrystal-saturn", 
          config={
              "platform": "saturn_cloud_a100",
              "gpu_count": torch.cuda.device_count(),
              "batch_size": 128
          })
```

### **ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§**
```bash
# GPU ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
watch -n 1 nvidia-smi

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
python -c "
import torch
for i in range(torch.cuda.device_count()):
    print(f'GPU {i}: {torch.cuda.memory_allocated(i)/1e9:.1f}GB / {torch.cuda.max_memory_allocated(i)/1e9:.1f}GB')
"
```

## ğŸ¯ **Best Practices**

1. **ì‹¤í—˜ ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸**:
   - [ ] GPU ë©”ëª¨ë¦¬ í™•ì¸
   - [ ] ë°ì´í„° ì—…ë¡œë“œ ì™„ë£Œ  
   - [ ] í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
   - [ ] Auto-shutdown í™œì„±í™”

2. **ì‹¤í–‰ ì¤‘ ëª¨ë‹ˆí„°ë§**:
   - [ ] GPU ì‚¬ìš©ë¥  90%+ ìœ ì§€
   - [ ] ë©”ëª¨ë¦¬ leak ì—†ìŒ í™•ì¸
   - [ ] ë¡œê·¸ ì‹¤ì‹œê°„ í™•ì¸

3. **ì‹¤í—˜ í›„ ì •ë¦¬**:
   - [ ] ê²°ê³¼ ë‹¤ìš´ë¡œë“œ/ë°±ì—…
   - [ ] ì¸ìŠ¤í„´ìŠ¤ ìˆ˜ë™ ì¢…ë£Œ
   - [ ] ë¹„ìš© í™•ì¸

ì´ ì„¤ì •ìœ¼ë¡œ Saturn Cloudì—ì„œ íš¨ìœ¨ì ìœ¼ë¡œ SenticCrystal ì‹¤í—˜ì„ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!