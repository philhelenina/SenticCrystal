{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74410a7f",
   "metadata": {},
   "source": [
    "installing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079ddced",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76b581b4",
   "metadata": {},
   "source": [
    "set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc892979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "import os\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "HOME_DIR = Path(\"/home/jovyan/workspace/Lingua-Emoca/\")\n",
    "os.chdir(HOME_DIR)\n",
    "\n",
    "SAVE_DIR = HOME_DIR / 'scripts' / 'results' / 'multimodal_opensmile_bert' / 'experiments' / 'lstm_max_pool_pad'\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "NUM_EPOCHS = 200\n",
    "BATCH_SIZE = 16  # Reduced for stability\n",
    "LEARNING_RATE = 0.001  # Increased for better convergence\n",
    "WEIGHT_DECAY = 0.01  # Reduced\n",
    "\n",
    "# --- SEED GENERATION ---\n",
    "# Generate 10 truly random seeds for the experiment\n",
    "experiment_seeds = np.random.randint(0, 10000, size=10)\n",
    "logger.info(f\"Starting experiment with seeds: {experiment_seeds}\")\n",
    "\n",
    "def set_seed(seed):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911d2e7d",
   "metadata": {},
   "source": [
    "MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2742a75-4fe0-4542-a993-bf48c44e3c9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T18:46:50.218718Z",
     "iopub.status.busy": "2025-09-23T18:46:50.218405Z",
     "iopub.status.idle": "2025-09-23T18:48:59.537762Z",
     "shell.execute_reply": "2025-09-23T18:48:59.537307Z",
     "shell.execute_reply.started": "2025-09-23T18:46:50.218699Z"
    }
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss implementation for addressing class imbalance\"\"\"\n",
    "    def __init__(self, alpha=1.0, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, num_classes, dropout_rate=0.3):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        current_size = input_size\n",
    "        \n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.extend([\n",
    "                nn.Linear(current_size, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_size),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            current_size = hidden_size\n",
    "        \n",
    "        layers.append(nn.Linear(current_size, num_classes))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceb60cb",
   "metadata": {},
   "source": [
    "running experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02180a2b-ad0b-431b-914d-3df1d8633133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(X_data, y_labels):\n",
    "    all_acc = []\n",
    "    all_f1 = []\n",
    "    \n",
    "    for run_idx, seed in enumerate(experiment_seeds):\n",
    "        logger.info(f\"Run {run_idx+1}/10 | Seed: {seed}\")\n",
    "        set_seed(seed)\n",
    "        \n",
    "        # 1. Model Initialization\n",
    "        # input_size is the number of openSMILE features\n",
    "        model = SimpleMLP(input_size=X_data.shape[1], hidden_sizes=[256, 128], num_classes=len(np.unique(y_labels))).to(device)\n",
    "        \n",
    "        # 2. Optimizer with WEIGHT_DECAY (L2 Regularization)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "        criterion = FocalLoss().to(device)\n",
    "        \n",
    "        # [Insert DataLoading & Training Loop here...]\n",
    "        # After training, evaluate:\n",
    "        \n",
    "        model.eval()\n",
    "        # ... (Get predictions for test set) ...\n",
    "        \n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "        \n",
    "        all_acc.append(acc)\n",
    "        all_f1.append(f1)\n",
    "        \n",
    "        logger.info(f\"Seed {seed} Result -> Acc: {acc:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "    # Final Statistics\n",
    "    logger.info(\"=\"*30)\n",
    "    logger.info(f\"Final Accuracy: {np.mean(all_acc):.4f} +/- {np.std(all_acc):.4f}\")\n",
    "    logger.info(f\"Final F1-Score: {np.mean(all_f1):.4f} +/- {np.std(all_f1):.4f}\")\n",
    "    logger.info(\"=\"*30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
