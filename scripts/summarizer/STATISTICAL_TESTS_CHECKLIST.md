# í†µê³„ í…ŒìŠ¤íŠ¸ ì²´í¬ë¦¬ìŠ¤íŠ¸ ë° ê°€ì´ë“œ

## âœ… ì™„ë£Œëœ í†µê³„ í…ŒìŠ¤íŠ¸

### 1. ì£¼ìš” ê°€ì„¤ ê²€ì • (Main Hypothesis)
âœ… **TEST 1: Flat vs Hierarchical**
- **4-way**: p < 0.001, Cohen's d = 2.92 (large effect)
- **6-way**: p < 0.001, Cohen's d = 2.41 (large effect)
- **ê²°ë¡ **: Hierarchicalì´ í†µê³„ì ìœ¼ë¡œ ë§¤ìš° ìœ ì˜í•˜ê²Œ ìš°ìˆ˜í•¨

### 2. Encoder ë¹„êµ
âœ… **TEST 2: Encoder Comparison (Flat only)**
- sentence-roberta vs bert-base: p < 0.001, d = 1.87
- sentence-roberta vs roberta-base: p < 0.001, d = 0.87
- roberta-base vs bert-base: p < 0.001, d = 0.86
- **ê²°ë¡ **: sentence-roberta > roberta-base > bert-base

### 3. Aggregator ë¹„êµ
âœ… **TEST 3: Aggregator Comparison (Hierarchical only)**
- mean, attn, sum, expdecay: ìœ ì˜í•œ ì°¨ì´ ì—†ìŒ
- lstm aggregator: ìœ ì˜í•˜ê²Œ ë‚®ì€ ì„±ëŠ¥ (p < 0.001)
- **ê²°ë¡ **: mean/attn ê¶Œì¥, lstm ì œì™¸

### 4. Classifier ë¹„êµ
âœ… **TEST 4: Classifier Comparison**
- 4-way: MLPì™€ LSTM ê°„ ìœ ì˜í•œ ì°¨ì´ ì—†ìŒ
- 6-way flat: MLPê°€ ì•½ê°„ ìš°ìˆ˜ (p = 0.006)
- **ê²°ë¡ **: ë‘ classifier ëª¨ë‘ ì‚¬ìš© ê°€ëŠ¥

### 5. Layer Selection
âœ… **TEST 5: Layer Selection**
- 4-way hierarchical: avg_last4ê°€ ì•½ê°„ ìš°ìˆ˜ (p = 0.007)
- 6-way hierarchical: lastê°€ ì•½ê°„ ìš°ìˆ˜ (p = 0.001)
- **ê²°ë¡ **: Task-dependent

### 6. Pooling Strategy
âœ… **TEST 6: Pooling Strategy**
- mean poolingì´ ê°€ì¥ ì•ˆì •ì 
- wmean_pos_revë„ ë¹„ìŠ·í•œ ì„±ëŠ¥
- **ê²°ë¡ **: mean ë˜ëŠ” wmean_pos_rev ê¶Œì¥

---

## ğŸ”¬ ì¶”ê°€ë¡œ ìˆ˜í–‰í•´ì•¼ í•  í†µê³„ í…ŒìŠ¤íŠ¸

### 1. Multiple Comparison Correction âš ï¸ **ì¤‘ìš”**

í˜„ì¬ ë§ì€ pairwise comparisonì„ ìˆ˜í–‰í–ˆìœ¼ë¯€ë¡œ ë‹¤ì¤‘ ë¹„êµ ë³´ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.

#### ë°©ë²• 1: Bonferroni Correction
```python
from scipy.stats import mannwhitneyu

# Original p-values
p_values = [0.000220, 0.000116, ...]  # ìˆ˜í–‰í•œ ëª¨ë“  í…ŒìŠ¤íŠ¸ì˜ p-value

# Bonferroni correction
n_tests = len(p_values)
alpha = 0.05
bonferroni_threshold = alpha / n_tests

# Check significance
corrected_results = [p < bonferroni_threshold for p in p_values]
```

#### ë°©ë²• 2: Holm-Bonferroni (ë” ê¶Œì¥)
```python
from statsmodels.stats.multitest import multipletests

p_values = [...]  # ëª¨ë“  p-value ë¦¬ìŠ¤íŠ¸
reject, p_corrected, _, _ = multipletests(p_values, alpha=0.05, method='holm')
```

#### ë°©ë²• 3: FDR (False Discovery Rate)
```python
reject, p_corrected, _, _ = multipletests(p_values, alpha=0.05, method='fdr_bh')
```

**ê¶Œì¥ì‚¬í•­**: 
- Main hypothesis (Flat vs Hierarchical)ëŠ” p < 0.001ì´ë¯€ë¡œ ì–´ë–¤ ë³´ì •ì„ ì ìš©í•´ë„ ìœ ì˜í•¨
- Secondary analysisëŠ” Holm-Bonferroni ì ìš© ê¶Œì¥

---

### 2. Effect Size Confidence Intervals âš ï¸ **ê¶Œì¥**

Cohen's dì˜ ì‹ ë¢°êµ¬ê°„ì„ ê³„ì‚°í•˜ì—¬ íš¨ê³¼ í¬ê¸°ì˜ ë¶ˆí™•ì‹¤ì„±ì„ ì •ëŸ‰í™”í•©ë‹ˆë‹¤.

```python
from scipy import stats
import numpy as np

def cohens_d_ci(group1, group2, confidence=0.95):
    """Calculate Cohen's d with confidence interval"""
    
    n1, n2 = len(group1), len(group2)
    
    # Cohen's d
    mean_diff = np.mean(group1) - np.mean(group2)
    pooled_std = np.sqrt(((n1-1)*np.var(group1, ddof=1) + 
                          (n2-1)*np.var(group2, ddof=1)) / (n1+n2-2))
    d = mean_diff / pooled_std
    
    # Standard error of d
    se_d = np.sqrt((n1 + n2) / (n1 * n2) + d**2 / (2 * (n1 + n2)))
    
    # Confidence interval
    z = stats.norm.ppf((1 + confidence) / 2)
    ci_lower = d - z * se_d
    ci_upper = d + z * se_d
    
    return d, (ci_lower, ci_upper)

# Example usage
hier_data = [...]  # Best hierarchical results
flat_data = [...]  # Best flat results

d, (ci_lower, ci_upper) = cohens_d_ci(hier_data, flat_data)
print(f"Cohen's d = {d:.3f}, 95% CI [{ci_lower:.3f}, {ci_upper:.3f}]")
```

---

### 3. Bootstrap Analysis âš ï¸ **ê¶Œì¥**

Bootstrapì„ ì‚¬ìš©í•˜ì—¬ í‰ê·  ì°¨ì´ì˜ ì‹ ë¢°êµ¬ê°„ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

```python
from scipy.stats import bootstrap

def bootstrap_mean_difference(group1, group2, n_bootstrap=10000):
    """Bootstrap confidence interval for mean difference"""
    
    def statistic(x, y):
        return np.mean(x) - np.mean(y)
    
    # Bootstrap
    rng = np.random.default_rng()
    res = bootstrap(
        (group1, group2),
        statistic,
        n_resamples=n_bootstrap,
        confidence_level=0.95,
        random_state=rng,
        method='percentile'
    )
    
    return res.confidence_interval

# Example
hier_data = df_raw[(df_raw['task']=='4way') & (df_raw['type']=='hierarchical') & 
                   ...]['weighted_f1'].values
flat_data = df_raw[(df_raw['task']=='4way') & (df_raw['type']=='flat') & 
                   ...]['weighted_f1'].values

ci = bootstrap_mean_difference(hier_data, flat_data)
print(f"Mean difference 95% CI: [{ci.low:.4f}, {ci.high:.4f}]")
```

---

### 4. Power Analysis âš ï¸ **ì„ íƒì **

í˜„ì¬ ìƒ˜í”Œ í¬ê¸°(n=10)ê°€ ì¶©ë¶„í•œì§€ ì‚¬í›„ ê²€ì •ë ¥ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

```python
from statsmodels.stats.power import ttest_power

def post_hoc_power(group1, group2, alpha=0.05):
    """Calculate post-hoc statistical power"""
    
    # Cohen's d
    n1, n2 = len(group1), len(group2)
    d = cohens_d(group1, group2)
    
    # Calculate power
    power = ttest_power(
        effect_size=abs(d),
        nobs=(n1 + n2) / 2,
        alpha=alpha,
        alternative='two-sided'
    )
    
    return power

# Example
power = post_hoc_power(hier_data, flat_data)
print(f"Statistical Power: {power:.4f}")
# If power > 0.8, sample size is adequate
```

---

### 5. Normality Test (ì„ íƒì )

Mann-Whitney U testëŠ” non-parametricì´ì§€ë§Œ, ë°ì´í„°ê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
from scipy.stats import shapiro

def test_normality(data, name="Data"):
    """Test for normality using Shapiro-Wilk test"""
    
    stat, p_value = shapiro(data)
    
    print(f"{name}:")
    print(f"  Shapiro-Wilk statistic: {stat:.4f}")
    print(f"  P-value: {p_value:.4f}")
    
    if p_value > 0.05:
        print(f"  âœ… Data appears normally distributed (p > 0.05)")
    else:
        print(f"  âš ï¸  Data may not be normally distributed (p < 0.05)")
    
    return p_value

# Test all groups
test_normality(hier_4way_data, "4-way Hierarchical")
test_normality(flat_4way_data, "4-way Flat")
```

---

### 6. Homogeneity of Variance Test (ì„ íƒì )

```python
from scipy.stats import levene

def test_homogeneity(group1, group2):
    """Test for homogeneity of variance using Levene's test"""
    
    stat, p_value = levene(group1, group2)
    
    print(f"Levene's Test:")
    print(f"  Statistic: {stat:.4f}")
    print(f"  P-value: {p_value:.4f}")
    
    if p_value > 0.05:
        print(f"  âœ… Variances are homogeneous (p > 0.05)")
    else:
        print(f"  âš ï¸  Variances are not homogeneous (p < 0.05)")
    
    return p_value

test_homogeneity(hier_data, flat_data)
```

---

### 7. Kruskal-Wallis Test (ë‹¤ì¤‘ ê·¸ë£¹ ë¹„êµ)

ì„¸ ê°œ ì´ìƒì˜ ê·¸ë£¹ì„ ë™ì‹œì— ë¹„êµí•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.

```python
from scipy.stats import kruskal

def kruskal_wallis_test(*groups):
    """Kruskal-Wallis H-test for multiple groups"""
    
    stat, p_value = kruskal(*groups)
    
    print(f"Kruskal-Wallis H-test:")
    print(f"  H-statistic: {stat:.4f}")
    print(f"  P-value: {p_value:.6f}")
    
    if p_value < 0.05:
        print(f"  *** At least one group is significantly different")
    else:
        print(f"  No significant difference among groups")
    
    return p_value

# Example: Compare all encoders
bert_data = df_raw[(df_raw['encoder']=='bert-base') & ...]['weighted_f1'].values
roberta_data = df_raw[(df_raw['encoder']=='roberta-base') & ...]['weighted_f1'].values
sroberta_data = df_raw[(df_raw['encoder']=='sentence-roberta') & ...]['weighted_f1'].values

kruskal_wallis_test(bert_data, roberta_data, sroberta_data)
```

---

### 8. Friedman Test (ë°˜ë³µ ì¸¡ì •)

ë™ì¼í•œ configurationì—ì„œ ì—¬ëŸ¬ seedì˜ ê²°ê³¼ë¥¼ ë¹„êµí•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.

```python
from scipy.stats import friedmanchisquare

def friedman_test(data_matrix):
    """
    Friedman test for repeated measures
    
    data_matrix: shape (n_configurations, n_seeds)
    """
    
    stat, p_value = friedmanchisquare(*data_matrix.T)
    
    print(f"Friedman Test:")
    print(f"  Chi-square statistic: {stat:.4f}")
    print(f"  P-value: {p_value:.6f}")
    
    return p_value

# Example: Compare different seeds for best config
best_config_data = df_raw[
    (df_raw['encoder']=='sentence-roberta') & 
    (df_raw['layer']=='last') & 
    ...
].pivot(index='seed', columns='task', values='weighted_f1')

friedman_test(best_config_data.values)
```

---

## ğŸ“Š ì¶”ê°€ ë¶„ì„ ê¶Œì¥ ì‚¬í•­

### 9. Confusion Matrix Analysis
```python
from sklearn.metrics import confusion_matrix
import seaborn as sns

def plot_confusion_matrix(y_true, y_pred, classes):
    """Plot confusion matrix"""
    
    cm = confusion_matrix(y_true, y_pred)
    
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=classes, yticklabels=classes)
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.title('Confusion Matrix')
    plt.tight_layout()
    plt.savefig('confusion_matrix.png', dpi=300)
```

### 10. Error Analysis by Class
```python
def analyze_per_class_performance(results_dict):
    """Analyze performance for each class"""
    
    for class_name, metrics in results_dict.items():
        print(f"\nClass: {class_name}")
        print(f"  Precision: {metrics['precision']:.4f}")
        print(f"  Recall: {metrics['recall']:.4f}")
        print(f"  F1-Score: {metrics['f1']:.4f}")
        print(f"  Support: {metrics['support']}")
```

### 11. Computational Cost Analysis
```python
def analyze_computational_cost():
    """Compare training time and memory usage"""
    
    # Load timing data
    flat_time = [...]
    hier_time = [...]
    
    print(f"Average Training Time:")
    print(f"  Flat: {np.mean(flat_time):.2f}s Â± {np.std(flat_time):.2f}s")
    print(f"  Hierarchical: {np.mean(hier_time):.2f}s Â± {np.std(hier_time):.2f}s")
    
    # Statistical test
    u, p = mannwhitneyu(hier_time, flat_time)
    print(f"  Mann-Whitney U test: p = {p:.4f}")
```

### 12. Learning Curve Analysis
```python
def plot_learning_curves(train_losses, val_losses):
    """Plot training and validation curves"""
    
    plt.figure(figsize=(10, 4))
    
    plt.subplot(1, 2, 1)
    plt.plot(train_losses, label='Train')
    plt.plot(val_losses, label='Validation')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.title('Loss Curves')
    
    plt.subplot(1, 2, 2)
    plt.plot(val_f1_scores)
    plt.xlabel('Epoch')
    plt.ylabel('F1 Score')
    plt.title('Validation F1')
    
    plt.tight_layout()
    plt.savefig('learning_curves.png', dpi=300)
```

---

## ğŸ¯ ìš°ì„ ìˆœìœ„ë³„ ê¶Œì¥ ì‚¬í•­

### ğŸ”´ **High Priority (í•„ìˆ˜)**
1. âœ… Multiple comparison correction (Bonferroni/Holm)
2. âœ… Effect size confidence intervals
3. âš ï¸ Bootstrap analysis for robustness

### ğŸŸ¡ **Medium Priority (ê¶Œì¥)**
4. âš ï¸ Power analysis (sample size adequacy)
5. âš ï¸ Confusion matrix analysis
6. âš ï¸ Per-class performance analysis

### ğŸŸ¢ **Low Priority (ì„ íƒ)**
7. Normality tests (ì´ë¯¸ non-parametric ì‚¬ìš©)
8. Homogeneity of variance tests
9. Computational cost comparison
10. Learning curve analysis

---

## ğŸ“ ë…¼ë¬¸ ì‘ì„± ì²´í¬ë¦¬ìŠ¤íŠ¸

### Results Section
- [x] Report best configurations with mean Â± std
- [x] Include p-values and effect sizes
- [x] Provide comparison tables
- [ ] Add confusion matrices
- [ ] Include per-class performance

### Statistical Reporting
- [x] Report test statistics (U, p-value)
- [x] Report effect sizes (Cohen's d)
- [ ] Report confidence intervals
- [ ] Apply multiple comparison correction
- [ ] Report statistical power

### Figures
- [x] Bar chart (Flat vs Hier)
- [x] Box plot (seed variance)
- [x] Encoder comparison
- [x] Aggregator comparison
- [x] Heatmap (top configurations)
- [ ] Confusion matrices
- [ ] Learning curves

### Tables
- [x] Best configurations (Top 5)
- [x] Statistical test results
- [ ] Ablation study results
- [ ] Computational cost comparison

---

## ğŸ’¡ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ì˜ˆì‹œ

```python
#!/usr/bin/env python3
"""
Complete statistical testing with corrections
"""

import pandas as pd
import numpy as np
from scipy import stats
from statsmodels.stats.multitest import multipletests

# Load data
df_raw = pd.read_csv('all_results_combined.csv')

# Collect all p-values from pairwise tests
p_values = []
test_names = []

# Main hypothesis
for task in ['4way', '6way']:
    # Get best configs...
    u, p = stats.mannwhitneyu(hier_data, flat_data, alternative='greater')
    p_values.append(p)
    test_names.append(f"Hier vs Flat ({task})")

# Encoder comparisons
encoders = ['bert-base', 'roberta-base', 'sentence-roberta']
for i in range(len(encoders)):
    for j in range(i+1, len(encoders)):
        # Get data...
        u, p = stats.mannwhitneyu(data1, data2, alternative='greater')
        p_values.append(p)
        test_names.append(f"{encoders[j]} vs {encoders[i]}")

# Apply Holm-Bonferroni correction
reject, p_corrected, _, _ = multipletests(p_values, alpha=0.05, method='holm')

# Print results
print("Test Results with Holm-Bonferroni Correction:")
print("-" * 80)
for name, p_orig, p_corr, sig in zip(test_names, p_values, p_corrected, reject):
    sig_str = "***" if sig else "n.s."
    print(f"{name:40s} p={p_orig:.6f} -> {p_corr:.6f} {sig_str}")
```

---

## âœ… ìµœì¢… ì²´í¬ë¦¬ìŠ¤íŠ¸

### ë°ì´í„° ë¶„ì„
- [x] 1,520 experiments collected
- [x] Statistical tests performed
- [x] Effect sizes calculated
- [ ] Multiple comparison correction
- [ ] Confidence intervals

### ì‹œê°í™”
- [x] Main results plot
- [x] Comparison plots
- [x] Heatmaps
- [ ] Confusion matrices

### ë…¼ë¬¸ ìë£Œ
- [x] LaTeX tables
- [x] Statistical test summary
- [ ] Supplementary materials

ì´ì œ ì´ ê°€ì´ë“œë¥¼ ë”°ë¼ ì¶”ê°€ í†µê³„ ë¶„ì„ì„ ìˆ˜í–‰í•˜ë©´ ë©ë‹ˆë‹¤!
