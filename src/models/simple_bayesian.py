"""
Simple Bayesian Implementation for Config146.

ê°„ë‹¨í•œ ë“œë¡­ì•„ì›ƒ ê¸°ë°˜ ë² ì´ì§€ì•ˆ ê·¼ì‚¬ë¡œ ì‹œì‘.
"""

import torch
import torch.nn as nn
import numpy as np
from typing import Tuple
import logging

logger = logging.getLogger(__name__)

class SimpleBayesianLSTM(nn.Module):
    """
    ê°„ë‹¨í•œ ë² ì´ì§€ì•ˆ LSTM - ë“œë¡­ì•„ì›ƒìœ¼ë¡œ ë¶ˆí™•ì‹¤ì„± ê·¼ì‚¬.
    
    Monte Carlo Dropoutì„ ì‚¬ìš©í•´ì„œ ë² ì´ì§€ì•ˆ íš¨ê³¼ë¥¼ ë‚¸ë‹¤.
    """
    
    def __init__(self, input_size: int = 768, hidden_size: int = 256, 
                 num_layers: int = 2, dropout: float = 0.3):
        super().__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.dropout = dropout
        
        # ê¸°ë³¸ LSTM
        self.lstm = nn.LSTM(
            input_size, hidden_size, num_layers, 
            batch_first=True, dropout=dropout if num_layers > 1 else 0
        )
        
        # ì¶”ê°€ ë“œë¡­ì•„ì›ƒ ë ˆì´ì–´ë“¤ (ë² ì´ì§€ì•ˆ ê·¼ì‚¬ìš©)
        self.dropout_layers = nn.ModuleList([
            nn.Dropout(dropout) for _ in range(3)  # ì—¬ëŸ¬ ë ˆì´ì–´ì— ë“œë¡­ì•„ì›ƒ
        ])
        
        # Context attention
        self.attention = nn.MultiheadAttention(hidden_size, 8, batch_first=True, dropout=dropout)
        
        logger.info(f"âœ… SimpleBayesianLSTM: {input_size}â†’{hidden_size}, dropout={dropout}")
    
    def forward(self, x: torch.Tensor, n_samples: int = 10, return_uncertainty: bool = True) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Forward pass with uncertainty estimation.
        
        Args:
            x: (batch_size, seq_len, input_size)
            n_samples: ëª‡ ë²ˆ ìƒ˜í”Œë§í• ì§€
            return_uncertainty: ë¶ˆí™•ì‹¤ì„±ë„ ë¦¬í„´í• ì§€
            
        Returns:
            mean_output: í‰ê·  ì˜ˆì¸¡
            uncertainty: ë¶ˆí™•ì‹¤ì„± (ë¶„ì‚°)
        """
        if not return_uncertainty or not self.training:
            # ì¼ë°˜ forward (ë“œë¡­ì•„ì›ƒ ì—†ì´)
            self.eval()
            lstm_out, _ = self.lstm(x)
            attended_out, _ = self.attention(lstm_out, lstm_out, lstm_out)
            output = attended_out[:, -1, :]  # ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í…
            return output, None
        
        # ë² ì´ì§€ì•ˆ ìƒ˜í”Œë§
        sample_outputs = []
        
        for _ in range(n_samples):
            # ë§¤ë²ˆ ë‹¤ë¥¸ ë“œë¡­ì•„ì›ƒ íŒ¨í„´ìœ¼ë¡œ forward
            self.train()  # ë“œë¡­ì•„ì›ƒ í™œì„±í™”
            
            # LSTM forward
            lstm_out, _ = self.lstm(x)
            
            # ì—¬ëŸ¬ ë‹¨ê³„ì—ì„œ ë“œë¡­ì•„ì›ƒ ì ìš©
            for dropout_layer in self.dropout_layers:
                lstm_out = dropout_layer(lstm_out)
            
            # Attention
            attended_out, _ = self.attention(lstm_out, lstm_out, lstm_out)
            
            # ë§ˆì§€ë§‰ íƒ€ì„ìŠ¤í… ì¶œë ¥
            output = attended_out[:, -1, :]
            sample_outputs.append(output)
        
        # í†µê³„ ê³„ì‚°
        sample_outputs = torch.stack(sample_outputs)  # (n_samples, batch_size, hidden_size)
        mean_output = torch.mean(sample_outputs, dim=0)
        uncertainty = torch.var(sample_outputs, dim=0)  # ì˜ˆì¸¡ ë¶ˆí™•ì‹¤ì„±
        
        return mean_output, uncertainty

class SimpleBayesianClassifier(nn.Module):
    """
    Config146ìš© ê°„ë‹¨í•œ ë² ì´ì§€ì•ˆ ë¶„ë¥˜ê¸°.
    """
    
    def __init__(self, input_size: int = 256, num_classes: int = 4, dropout: float = 0.5):
        super().__init__()
        
        # ë² ì´ì§€ì•ˆ Context LSTM
        self.bayesian_lstm = SimpleBayesianLSTM(768, input_size, dropout=dropout)
        
        # ë¶„ë¥˜ í—¤ë“œ (ì—¬ëŸ¬ ë“œë¡­ì•„ì›ƒ í¬í•¨)
        self.classifier = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(input_size, input_size // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(input_size // 2, num_classes)
        )
        
        logger.info(f"âœ… SimpleBayesianClassifier: {input_size}â†’{num_classes}")
    
    def forward(self, x: torch.Tensor, n_samples: int = 10) -> dict:
        """
        ë² ì´ì§€ì•ˆ ì˜ˆì¸¡ with ë¶ˆí™•ì‹¤ì„±.
        
        Returns:
            dict with 'prediction', 'uncertainty', 'confidence'
        """
        # ë² ì´ì§€ì•ˆ LSTM forward
        lstm_out, lstm_uncertainty = self.bayesian_lstm(x, n_samples=n_samples)
        
        # ë¶„ë¥˜ ì˜ˆì¸¡ë„ ìƒ˜í”Œë§
        classification_samples = []
        for _ in range(n_samples):
            self.train()  # ë“œë¡­ì•„ì›ƒ í™œì„±í™”
            logits = self.classifier(lstm_out)
            probs = torch.softmax(logits, dim=-1)
            classification_samples.append(probs)
        
        # ë¶„ë¥˜ í†µê³„
        classification_samples = torch.stack(classification_samples)
        mean_probs = torch.mean(classification_samples, dim=0)
        classification_uncertainty = torch.var(classification_samples, dim=0)
        
        # ì‹ ë¢°ë„ ê³„ì‚° (ë†’ì€ í™•ë¥  + ë‚®ì€ ë¶ˆí™•ì‹¤ì„±)
        max_prob, predicted_class = torch.max(mean_probs, dim=-1)
        total_uncertainty = torch.sum(classification_uncertainty, dim=-1)  # ì´ ë¶ˆí™•ì‹¤ì„±
        confidence = max_prob / (1.0 + total_uncertainty)  # ì •ê·œí™”ëœ ì‹ ë¢°ë„
        
        return {
            'prediction': predicted_class,
            'probabilities': mean_probs,
            'uncertainty': total_uncertainty,
            'confidence': confidence,
            'lstm_uncertainty': lstm_uncertainty
        }
    
    def predict_with_confidence(self, x: torch.Tensor, confidence_threshold: float = 0.7) -> dict:
        """
        ì‹ ë¢°ë„ ê¸°ë°˜ ì˜ˆì¸¡.
        
        ë‚®ì€ ì‹ ë¢°ë„ë©´ "uncertain" í”Œë˜ê·¸ í¬í•¨.
        """
        result = self.forward(x)
        
        # ì‹ ë¢°ë„ ì²´í¬
        is_confident = result['confidence'] > confidence_threshold
        
        result.update({
            'is_confident': is_confident,
            'needs_human_review': ~is_confident
        })
        
        return result

def demonstrate_bayesian_difference():
    """ë² ì´ì§€ì•ˆê³¼ ì¼ë°˜ LSTM ì°¨ì´ ì‹œì—°."""
    
    # ê°€ì§œ ë°ì´í„° (4-turn context)
    batch_size, seq_len, embed_dim = 32, 4, 768
    x = torch.randn(batch_size, seq_len, embed_dim)
    
    print("ğŸ” ë² ì´ì§€ì•ˆ vs ì¼ë°˜ LSTM ë¹„êµ")
    print("="*50)
    
    # 1. ì¼ë°˜ LSTM
    regular_lstm = nn.LSTM(768, 256, batch_first=True)
    regular_out, _ = regular_lstm(x)
    regular_final = regular_out[:, -1, :]  # ë§ˆì§€ë§‰ ì¶œë ¥
    
    print(f"ì¼ë°˜ LSTM:")
    print(f"  ì¶œë ¥ í˜•íƒœ: {regular_final.shape}")
    print(f"  í‰ê· : {regular_final.mean().item():.4f}")
    print(f"  í‘œì¤€í¸ì°¨: {regular_final.std().item():.4f}")
    print(f"  ë¶ˆí™•ì‹¤ì„±: ì¸¡ì • ë¶ˆê°€ âŒ")
    
    # 2. ë² ì´ì§€ì•ˆ LSTM  
    bayesian_lstm = SimpleBayesianLSTM(768, 256)
    bayesian_out, uncertainty = bayesian_lstm(x, n_samples=10)
    
    print(f"\në² ì´ì§€ì•ˆ LSTM:")
    print(f"  ì¶œë ¥ í˜•íƒœ: {bayesian_out.shape}")
    print(f"  í‰ê· : {bayesian_out.mean().item():.4f}")
    print(f"  í‘œì¤€í¸ì°¨: {bayesian_out.std().item():.4f}")
    print(f"  ë¶ˆí™•ì‹¤ì„±: {uncertainty.mean().item():.4f} âœ…")
    
    # 3. ë¶„ë¥˜ ë¹„êµ
    bayesian_classifier = SimpleBayesianClassifier(256, 4)
    result = bayesian_classifier.predict_with_confidence(x)
    
    print(f"\në² ì´ì§€ì•ˆ ë¶„ë¥˜:")
    print(f"  ì˜ˆì¸¡ í´ë˜ìŠ¤: {result['prediction'][0].item()}")
    print(f"  ì‹ ë¢°ë„: {result['confidence'][0].item():.4f}")
    print(f"  ë¶ˆí™•ì‹¤ì„±: {result['uncertainty'][0].item():.4f}")
    print(f"  ë¦¬ë·° í•„ìš”: {result['needs_human_review'][0].item()}")

if __name__ == "__main__":
    demonstrate_bayesian_difference()